{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from urllib.request import urlopen, urlretrieve\n",
    "from bs4 import BeautifulSoup\n",
    "import regex as re\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "\n",
    "site = 'https://kia-rio.net/forum/'\n",
    "folder_names = [\n",
    "    ['0', 'Отзывы', 'f=117', 'KIA RIO'],\n",
    "    ['1', 'Двигатель', 'f=53', 'KIA RIO'],\n",
    "    ['2', 'Трансмиссия', 'f=51', 'KIA RIO'],\n",
    "    ['3', 'Ходовая', 'f=75', 'KIA RIO'],\n",
    "    ['4', 'Колеса', 'f=118', 'KIA RIO'],\n",
    "    ['5', 'Электрооборудование', 'f=52', 'KIA RIO'],\n",
    "    ['6', 'Мультимедиа', 'f=64', 'KIA RIO'],\n",
    "    ['7', 'Кузов', 'f=83', 'KIA RIO'],\n",
    "    ['8', 'Жидкости', '', 'KIA RIO'],\n",
    "    ['9', 'Сервис', 'f=19', 'KIA RIO'],\n",
    "    ['10', 'Эксплуатация', '', 'KIA RIO'],\n",
    "    ['11', 'Тюнинг', 'f=84', 'KIA RIO'],\n",
    "    ['12', 'Двигатель_система_охлаждения', 'f=59', 'KIA RIO'],\n",
    "    ['13', 'Кузов_кондиционер', 'f=74', 'KIA RIO'],\n",
    "    ['14', 'Сервис_ремонт', 'f=20', 'KIA RIO'],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На странице раздела у нас две задачи:  \n",
    "1. Найти и сохранить все ссылки на темы\n",
    "2. Найти ссылку на следующую страницу раздела или убедиться, что это последняя страница раздела."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_themes(site, folder):\n",
    "    \n",
    "    site_address = site + 'forumdisplay.php?' + folder\n",
    "    themes = []\n",
    "    flag = True\n",
    "\n",
    "    while flag:\n",
    "    \n",
    "        resp = urlopen(site_address) # скачиваем файл\n",
    "        html = resp.read().decode('utf-8') # считываем содержимое\n",
    "        soup = BeautifulSoup(html, 'html.parser') # делаем суп\n",
    "    \n",
    "        flag = False\n",
    "        for link in soup.find_all('a'):\n",
    "            if link.has_attr('href') and link.has_attr('id'):\n",
    "                if re.search('thread_title_', str(link.get('id'))):\n",
    "                    s = link.get('href')\n",
    "                    s = site + 'showthread.php?' + re.search('t=[0-9]+', s)[0]\n",
    "                    themes.append([link.get_text(), s])\n",
    "            if not flag and link.has_attr('href') and link.has_attr('rel'):\n",
    "                if link.get('rel')[0] == 'next':\n",
    "                    flag = True\n",
    "                    s = link.get('href')\n",
    "                    site_address = site + 'forumdisplay.php?' + re.search(folder + '[\\S]+', s)[0]\n",
    "    return themes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На странице темы у нас три задачи:  \n",
    "1. Понять, что являеся шапкой темы (это сообщение #1), прочитать ее только один раз - НЕ СДЕЛАНО\n",
    "2. Прочитать и сохранить все сообщения (кроме шапки) на странице темы\n",
    "3. Найти ссылку на следующую страницу темы или убедиться, что это последняя страница темы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_messages(site, themes):\n",
    "\n",
    "    messages = []\n",
    "    for theme in tqdm(themes):\n",
    "        \n",
    "        print(theme[0])\n",
    "        theme_address = theme[1]\n",
    "        thread = re.search('t=[0-9]+', theme_address)[0]\n",
    "        flag = True\n",
    "        theme_messages = []\n",
    "        post_data = {}\n",
    "\n",
    "        while flag:\n",
    "            resp = urlopen(theme_address) # скачиваем файл\n",
    "            html = resp.read().decode('utf-8', errors='ignore') # считываем содержимое\n",
    "            soup = BeautifulSoup(html, 'html.parser') # делаем суп\n",
    "            #Собираем сами сообщения\n",
    "            divs = soup.find_all('div')\n",
    "            for div in divs:\n",
    "                if div.has_attr('id') and re.search('(postmenu_)([0-9]+)(_menu)', div['id']):\n",
    "                    continue\n",
    "                if div.has_attr('id') and re.search('postmenu_', div['id']):\n",
    "                    author = div.find('a')\n",
    "                    if author != None:\n",
    "                        author_id = re.search('u=[0-9]+', author['href'])[0]\n",
    "                        author_nickname = author.get_text()\n",
    "                    else:\n",
    "                        author_id = div.get_text().strip()\n",
    "                        author_nickname = 'Guest'\n",
    "                    post_data[re.sub('postmenu_', '', div['id'])] = [author_id, author_nickname]\n",
    "                if div.has_attr('id') and re.search('post_message_', div['id']):\n",
    "                    message_id = re.sub('post_message_', '', div['id'])\n",
    "                    if message_id in post_data:\n",
    "                        author = post_data[message_id][1]\n",
    "                    else:\n",
    "                        author = ''\n",
    "                    for item in div.children:\n",
    "                        if item.name == 'div':\n",
    "                            if item.has_attr('class') and item['class'][0] == 'smallfont':\n",
    "                                post_date = item.get_text().strip()\n",
    "                            item.clear()\n",
    "                    s = re.sub('[\\']+', '`', div.get_text())\n",
    "                    s = '\\'' + re.sub('[\\s]+', ' ', s).strip() + '\\''\n",
    "                    if len(s) > 2:\n",
    "                        theme_messages.append([post_date, author, s])\n",
    "            # В отдельном цикле ищем ссылку на следующую страницу темы (это 'a' с атрибутом 'rel', равным 'next')\n",
    "            # Если ссылка найдена, \n",
    "            flag = False \n",
    "            for link in soup.find_all('a'):\n",
    "                if not flag and link.has_attr('href') and link.has_attr('rel'):\n",
    "                    if link.get('rel')[0] == 'next':\n",
    "                        flag = True\n",
    "                        s = link.get('href')\n",
    "                        theme_address = site + 'showthread.php?' + re.search(thread + '[\\S]+', s)[0]\n",
    "        messages.append(['\\''+ theme[0] + '\\'', theme_messages])\n",
    "        \n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_messages(messages, folder_name):\n",
    "    f_name = os.path.join(folder_name[3], folder_name[3] + '_' + folder_name[0] + '_' + folder_name[1] + '.csv')\n",
    "    with open(f_name, 'w', encoding='utf-8') as ouf:\n",
    "        ouf.write('Code,Folder,Theme,Author,Date,Message\\n')\n",
    "        for message in messages:\n",
    "            header = folder_name[:2] + [message[0]]\n",
    "            for item in message[1]:\n",
    "                ouf.write(','.join(header + item))\n",
    "                ouf.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper(site, folder_names):\n",
    "    for folder_name in folder_names:\n",
    "        if len(folder_name[2]) > 0:\n",
    "            folder = folder_name[2]\n",
    "            current_themes = collect_themes(site, folder)\n",
    "            current_messages = collect_messages(site, current_themes)\n",
    "            save_messages(current_messages, folder_name)\n",
    "    print('Information collected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scraper(site, folder_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#mess = pd.read_csv('./KIA RIO/KIA RIO_5_Электрооборудование.csv', quotechar=\"'\")\n",
    "#mess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
