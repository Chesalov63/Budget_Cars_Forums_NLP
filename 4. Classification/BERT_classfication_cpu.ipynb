{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGnlRWvkY-2c"
   },
   "source": [
    "<h1><center> Классификация текстов с помощью модели BERT</center></h1>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-S077NiGIrid"
   },
   "source": [
    "На этом занятии мы вернемся к задаче классификации текстов. \n",
    "\n",
    "В качестве данных будем использовать отзывы на приложения в Google Play. \n",
    "Данные будут содержать тексты отзывов и оценки, которые пользователи поставили приложениям (от 1 до 5).\n",
    "\n",
    "Наша задача -- построить модель, которая сможет предсказать оценку пользователя на основе текста его отзыва.\n",
    "\n",
    "Для решения этой задачи мы будем использовать модель BERT, основанную на архитектуре Transformer. \n",
    "\n",
    "Мы научимся:\n",
    "- Обрабатывать данные для модели BERT\n",
    "- Дообучать модель BERT на нашем наборе данных\n",
    "- Использовать обученную модель для оценки текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 762,
     "status": "ok",
     "timestamp": 1604763837913,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "68yw4KZZRhoQ",
    "outputId": "5e9b3083-a12e-4c2f-e069-8e3dbe5a5b33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score,accuracy_score,classification_report,confusion_matrix\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter,defaultdict\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ufzPdoTtNikq"
   },
   "source": [
    "## Загрузка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnMnfH86v9OZ"
   },
   "source": [
    "Мы будем использовать библиотеку [Transformers](https://huggingface.co/transformers/) от Hugging Face "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 3726,
     "status": "ok",
     "timestamp": 1604763843027,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "Kj_7Tz0-pK69"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 6212,
     "status": "ok",
     "timestamp": 1604763846184,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "Jjsbi1u3QFEM"
   },
   "outputs": [],
   "source": [
    "!pip install -qq transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5581,
     "status": "ok",
     "timestamp": 1604763846186,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "AJqoaFpVpoM8",
    "outputId": "a2b3632e-7f32-4d85-f6f9-459a0a5e2f10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPython 3.8.5\n",
      "IPython 7.18.1\n",
      "\n",
      "numpy 1.19.1\n",
      "pandas 1.1.3\n",
      "torch 1.6.0+cpu\n",
      "transformers 3.4.0\n"
     ]
    }
   ],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -v -p numpy,pandas,torch,transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jcsWUODBwScb"
   },
   "source": [
    "Загрузим набор данных, который мы будем использовать для обучения и тестирования модели -- отзывы на приложения в Google Play.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6772,
     "status": "ok",
     "timestamp": 1604763849172,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "SgPRhuMzi9ot",
    "outputId": "996cf4f7-631b-45a9-9e11-4531846393bc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"gdown\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n",
      "\"gdown\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n"
     ]
    }
   ],
   "source": [
    "!gdown --id 1S6qMioqPJjyBLpLVz4gmRTnJHnjitnuV\n",
    "!gdown --id 1zdmewp7ayS4js4VtrJEHzAheSW-5NBZv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "executionInfo": {
     "elapsed": 675,
     "status": "ok",
     "timestamp": 1604763851777,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "UZfvRBwbNmdB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 1522,
     "status": "ok",
     "timestamp": 1604763853429,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "mUKLyKc7I6Qp"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./Data/reviews.csv\")\n",
    "# print(\"Количество примеров: \", df.shape[0])\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 454,
     "status": "ok",
     "timestamp": 1604763853432,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "dB2jE6am7Dpo",
    "outputId": "2ce1c70c-2171-47dd-9430-a6f461e46c7a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15746, 11)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 818,
     "status": "ok",
     "timestamp": 1604763855043,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "VA_wGSLQLKCh",
    "outputId": "cf1fd3e9-a087-4249-937d-d960627b357b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15746 entries, 0 to 15745\n",
      "Data columns (total 11 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   userName              15746 non-null  object\n",
      " 1   userImage             15746 non-null  object\n",
      " 2   content               15746 non-null  object\n",
      " 3   score                 15746 non-null  int64 \n",
      " 4   thumbsUpCount         15746 non-null  int64 \n",
      " 5   reviewCreatedVersion  13533 non-null  object\n",
      " 6   at                    15746 non-null  object\n",
      " 7   replyContent          7367 non-null   object\n",
      " 8   repliedAt             7367 non-null   object\n",
      " 9   sortOrder             15746 non-null  object\n",
      " 10  appId                 15746 non-null  object\n",
      "dtypes: int64(2), object(9)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPqoVTjJwpDF"
   },
   "source": [
    "Посмотрим на распределение классов в выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "executionInfo": {
     "elapsed": 798,
     "status": "ok",
     "timestamp": 1604763856803,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "Wwh_rW4Efhs3",
    "outputId": "0cc19711-20dc-443c-9022-ab66fefe043e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S:\\Anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(df.score)\n",
    "plt.xlabel('review score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdDtjvRsw9PT"
   },
   "source": [
    "Можно видеть, что данные несбалансированы. \n",
    "Теперь приведем метки классов к другому виду -- разделим их на 3 класса: негативные, нейтральные и позитивные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 1016,
     "status": "ok",
     "timestamp": 1604763859066,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "ei0xmdi1Chp0"
   },
   "outputs": [],
   "source": [
    "def to_sentiment(rating):\n",
    "  rating = int(rating)\n",
    "  if rating <= 2:\n",
    "    return 0\n",
    "  elif rating == 3:\n",
    "    return 1\n",
    "  else: \n",
    "    return 2\n",
    "\n",
    "df['sentiment'] = df.score.apply(to_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 674,
     "status": "ok",
     "timestamp": 1604763860340,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "V-155O-SFSqE"
   },
   "outputs": [],
   "source": [
    "class_names = ['negative', 'neutral', 'positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "executionInfo": {
     "elapsed": 1274,
     "status": "ok",
     "timestamp": 1604763861743,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "y3tY3ECJDPaz",
    "outputId": "88790992-e378-4f0e-c7e6-dcf0192e2b5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S:\\Anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.countplot(df.sentiment)\n",
    "plt.xlabel('review sentiment')\n",
    "ax.set_xticklabels(class_names);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptzUBxnkxPNx"
   },
   "source": [
    "В таком виде классы почти сбалансированы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9aHyGuTFgyPO"
   },
   "source": [
    "## Предобработка данных\n",
    "\n",
    "Для обучение модели BERT нам нужно обработать исходный текст определенным образом:\n",
    "* Добавить служебные символы для разделения предложений и классификации\n",
    "* Привести все последовательности к единой длине (используя padding)\n",
    "* Создать список (*attention mask*) из 0 и 1, в котором 0 будут соответствовать вспомогательным токенам (padding), а 1 $-$ настоящим.\n",
    "\n",
    "Будем использовать модели токенизации из библиотеки Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 695,
     "status": "ok",
     "timestamp": 1604763864571,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "E7Mj-0ne--5t"
   },
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NiLb-ltM-ZRz"
   },
   "source": [
    "Загрузим предобученный [BertTokenizer](https://huggingface.co/transformers/model_doc/bert.html#berttokenizer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 665,
     "status": "ok",
     "timestamp": 1604763866103,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "4PBU12c0QcmF"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 998,
     "status": "ok",
     "timestamp": 1604763867213,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "H3AfJSZ8NNLF"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CfrSbwTQ-wi_"
   },
   "source": [
    "Посмотрим, как работает модель токенизации для BERT.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 602,
     "status": "ok",
     "timestamp": 1604763870942,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "HZMitwrqm2eb"
   },
   "outputs": [],
   "source": [
    "sample_txt = 'When was I last outside? I am stuck at home for 2 weeks.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_txt = \"Once upon a midnight dreary, while I pondered, weak and weary, \\\n",
    "Over many a quaint and curious volume of forgotten lore — \\\n",
    "While I nodded, nearly napping, suddenly there came a tapping, \\\n",
    "As of some one gently rapping, rapping at my chamber door. \\\n",
    "'Tis some visitor,' I muttered, 'tapping at my chamber door— \\\n",
    "Only this and nothing more.'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 833,
     "status": "ok",
     "timestamp": 1604763872145,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "iTFhpHpsoWO7",
    "outputId": "86fa6a4f-4204-482b-fcda-8eff57c96de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sentence: When was I last outside? I am stuck at home for 2 weeks.\n",
      "   Tokens: ['When', 'was', 'I', 'last', 'outside', '?', 'I', 'am', 'stuck', 'at', 'home', 'for', '2', 'weeks', '.']\n",
      "Token IDs: [1332, 1108, 146, 1314, 1796, 136, 146, 1821, 5342, 1120, 1313, 1111, 123, 2277, 119]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sample_txt)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(f' Sentence: {sample_txt}')\n",
    "print(f'   Tokens: {tokens}')\n",
    "print(f'Token IDs: {token_ids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzbbKLR8lZbu"
   },
   "source": [
    "\n",
    "##  Специальные токены\n",
    "\n",
    "`[SEP]` - метка конца предложения\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 713,
     "status": "ok",
     "timestamp": 1604763875426,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "EXwz47bQvCbc",
    "outputId": "7b852b66-b5ee-4d80-9eb4-7b13aebf5d3b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[SEP]', 102)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sep_token, tokenizer.sep_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mip_eGeXwLFF"
   },
   "source": [
    "`[CLS]` - чтобы использовать модель BERT для классификации, мы должны добавить этот токен в начало каждого предложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 834,
     "status": "ok",
     "timestamp": 1604763880524,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "_6K4it5HwE6l",
    "outputId": "b6a03eff-b247-4c42-ecd8-fa14ce0ecbe4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[CLS]', 101)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.cls_token, tokenizer.cls_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qi6O-yEY09gl"
   },
   "source": [
    "Также существует специальный вспомогательный токен для выравнивания длин последовательностей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 818,
     "status": "ok",
     "timestamp": 1604763881724,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "Vx7gD5xf1AFK",
    "outputId": "10847ae2-4f45-45c6-be58-f045dfd7dab3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[PAD]', 0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token, tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GWCfijM0TWB"
   },
   "source": [
    "Кроме того, существует специальный токен для неизвестных слов, которые не встречались в обучающей выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 726,
     "status": "ok",
     "timestamp": 1604763882604,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "4cmfFsbEKQDT",
    "outputId": "6fb28bff-2b1d-4147-b02b-e704ac24555e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[UNK]', 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.unk_token, tokenizer.unk_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9ap7jdL0LYU"
   },
   "source": [
    "Вся эта предобработка может быть сделана с помощью метода encode_pluse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 537,
     "status": "ok",
     "timestamp": 1604763883619,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "Vea9edaaxSPO",
    "outputId": "faba3b1e-5ea5-4f9f-9567-502e55cd7c83"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "S:\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1938: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tokenizer.encode_plus(\n",
    "  sample_txt,\n",
    "  max_length=32,\n",
    "  add_special_tokens=True, # Добавить '[CLS]' и '[SEP]'\n",
    "  return_token_type_ids=False,\n",
    "  pad_to_max_length=True,\n",
    "  return_attention_mask=True,\n",
    "  return_tensors='pt',  # Вернуть тензор PyTorch\n",
    ")\n",
    "\n",
    "encoding.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 759,
     "status": "ok",
     "timestamp": 1604764577953,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "YzBmcOla0yQR",
    "outputId": "af926305-8cfd-4022-a3fc-55286b5539e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 101, 1332, 1108,  146, 1314, 1796,  136,  146, 1821, 5342, 1120, 1313,\n",
       "        1111,  123, 2277,  119,  102,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(encoding['input_ids'][0]))\n",
    "encoding['input_ids'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itAyVPsNdyc1"
   },
   "source": [
    "Такая же длина будет и у *attention mask*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 693,
     "status": "ok",
     "timestamp": 1604764580350,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "Wiv5LLiw03Ox",
    "outputId": "eaf8318a-5805-4e73-c2bc-86af80194cc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(encoding['attention_mask'][0]))\n",
    "encoding['attention_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kC0YgbzcFvpZ"
   },
   "source": [
    "Установим максимальную длину последовательности равной 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 702,
     "status": "ok",
     "timestamp": 1604763892456,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "t7xSmJtLuoxW"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 160"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XvvcoU6nurHy"
   },
   "source": [
    "Теперь создадим датасет PyTorch, который понадобится в дальнейшем для обучения модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 659,
     "status": "ok",
     "timestamp": 1604763895376,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "eUOzJ1ssSgnk"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 700,
     "status": "ok",
     "timestamp": 1604763897545,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "E2BPgRJ7YBK0"
   },
   "outputs": [],
   "source": [
    "class GPReviewDataset(Dataset):\n",
    "\n",
    "  def __init__(self, reviews, targets, tokenizer, max_len):\n",
    "    self.reviews = reviews\n",
    "    self.targets = targets\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.reviews)\n",
    "  \n",
    "  def __getitem__(self, item):\n",
    "    review = str(self.reviews[item])\n",
    "    target = self.targets[item]\n",
    "\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "      review,\n",
    "      add_special_tokens=True,\n",
    "      max_length=self.max_len,\n",
    "      return_token_type_ids=False,\n",
    "      pad_to_max_length=True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    return {\n",
    "      'review_text': review,\n",
    "      'input_ids': encoding['input_ids'].flatten(),\n",
    "      'attention_mask': encoding['attention_mask'].flatten(),\n",
    "      'targets': torch.tensor(target, dtype=torch.long)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2uwsvCYqDJK"
   },
   "source": [
    "Разделим данные на обучающую, тестовую и валидационную выборки "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 653,
     "status": "ok",
     "timestamp": 1604763901429,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "BAwIxL0xS-hO"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "RANDOM_SEED = 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 853,
     "status": "ok",
     "timestamp": 1604763902958,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "B-vWzoo81dvO"
   },
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.1, random_state=RANDOM_SEED)\n",
    "df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 715,
     "status": "ok",
     "timestamp": 1604763904706,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "xz3ZOQXVPCwh",
    "outputId": "2985f659-ed10-41d7-f0de-0e0bcb5d2349"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14171, 12), (787, 12), (788, 12))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_val.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4tQ1x-vqNab"
   },
   "source": [
    "Для обработки данных нам также нужно будет создать итераторы \n",
    "- train_data_loader - данные для обучения\n",
    "- val_data_loader - данные для валидации модели при обучении\n",
    "- test_data_loader - данные для тестирования модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 506,
     "status": "ok",
     "timestamp": 1604763907901,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "KEGqcvkuOuTX"
   },
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "  ds = GPReviewDataset(\n",
    "    reviews=df.content.to_numpy(),\n",
    "    targets=df.sentiment.to_numpy(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=max_len\n",
    "  )\n",
    "\n",
    "  return DataLoader(\n",
    "    ds,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=4\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 665,
     "status": "ok",
     "timestamp": 1604763909553,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "vODDxMKsPHqI"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6dlOptwqlhF"
   },
   "source": [
    "Посмотрим на пример одного батча из нашего итератора train_data_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1219,
     "status": "ok",
     "timestamp": 1604763913064,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "Y93ldSN47FeT",
    "outputId": "ba9798a0-bd6c-4db4-f0ae-66b2bbfe5461"
   },
   "outputs": [],
   "source": [
    "data = next(iter(train_data_loader))\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 792,
     "status": "ok",
     "timestamp": 1604764597055,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "IdU4YVqb7N8M",
    "outputId": "701d3fba-6efa-4d25-b350-e8b9975ffab3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 160])\n",
      "torch.Size([16, 160])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "print(data['input_ids'].shape)\n",
    "print(data['attention_mask'].shape)\n",
    "print(data['targets'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H63Y-TjyRC7S"
   },
   "source": [
    "## Классификация текстов по тональности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "440Nd31VTHER"
   },
   "source": [
    "\n",
    "В библиотеке Transformers от Hugging Face есть много моделей для разных задач: [BertForSequenceClassification](https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification), [BertForQuestionAnswering](https://huggingface.co/transformers/model_doc/bert.html#bertforquestionanswering) и другие. Все они являются надстройками над базовой моделью BERT.\n",
    "\n",
    "В данном случае мы будем использовать базовую модель [BertModel](https://huggingface.co/transformers/model_doc/bert.html#bertmodel) и реализуем на её основе свой классификатор текстов по тональности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "executionInfo": {
     "elapsed": 4610,
     "status": "ok",
     "timestamp": 1604764428434,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "0P41FayISNRI"
   },
   "outputs": [],
   "source": [
    "bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFE7YSbFdY4t"
   },
   "source": [
    "Попробуем использовать эту модель "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "executionInfo": {
     "elapsed": 626,
     "status": "ok",
     "timestamp": 1604764435481,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "s1aoFxbQSn15"
   },
   "outputs": [],
   "source": [
    "last_hidden_state, pooled_output = bert_model(\n",
    "  input_ids=encoding['input_ids'], \n",
    "  attention_mask=encoding['attention_mask']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mLLu8zmqbaHV"
   },
   "source": [
    "\n",
    "В переменную last_hidden_state теперь записана последовательность скрытых состояний последнего слоя модели. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 726,
     "status": "ok",
     "timestamp": 1604764438169,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "mUJHXNpIbcci",
    "outputId": "06d107c7-d345-4977-ad83-ffb08e7e8343"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 768])"
      ]
     },
     "execution_count": 139,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0o_NiS3WgOFf"
   },
   "source": [
    "Опишем наконец наш классификатор на основе BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "executionInfo": {
     "elapsed": 717,
     "status": "ok",
     "timestamp": 1604764443280,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "m_mRflxPl32F"
   },
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "\n",
    "  def __init__(self, n_classes):\n",
    "    super(SentimentClassifier, self).__init__()\n",
    "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "    self.drop = nn.Dropout(p=0.3)\n",
    "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "  \n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    _, pooled_output = self.bert(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask\n",
    "    )\n",
    "    output = self.drop(pooled_output)\n",
    "    return self.out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "executionInfo": {
     "elapsed": 662,
     "status": "ok",
     "timestamp": 1604764525455,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "i0yQnuSFsjDp"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "executionInfo": {
     "elapsed": 4969,
     "status": "ok",
     "timestamp": 1604764537276,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "o-8Ie6Qqa00x"
   },
   "outputs": [],
   "source": [
    "model = SentimentClassifier(len(class_names))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCPCFDLlKIQd"
   },
   "source": [
    "Перенесем на GPU один из наших батчей (для примера)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "executionInfo": {
     "elapsed": 737,
     "status": "ok",
     "timestamp": 1604764610241,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "vsTFqf_iWxL_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 707,
     "status": "ok",
     "timestamp": 1604764623984,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "mz7p__CqdaMO",
    "outputId": "4cd9334e-bf87-4f0b-c434-34239edc972b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 160])\n",
      "torch.Size([16, 160])\n"
     ]
    }
   ],
   "source": [
    "input_ids = data['input_ids'].to(device)\n",
    "attention_mask = data['attention_mask'].to(device)\n",
    "\n",
    "print(input_ids.shape) # batch size x seq length\n",
    "print(attention_mask.shape) # batch size x seq length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hr1EgkEtKOIB"
   },
   "source": [
    "Чтобы получить вероятности классов, мы применим SoftMax к выходу модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 740,
     "status": "ok",
     "timestamp": 1604764646992,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "2rTCj46Zamry",
    "outputId": "eddbdae5-ec59-4218-f9b3-bde9d76f5945"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3449, 0.2965, 0.3585],\n",
       "        [0.3350, 0.4480, 0.2170],\n",
       "        [0.3222, 0.4484, 0.2295],\n",
       "        [0.5071, 0.2751, 0.2178],\n",
       "        [0.4843, 0.2897, 0.2259],\n",
       "        [0.3892, 0.4460, 0.1648],\n",
       "        [0.3748, 0.2564, 0.3688],\n",
       "        [0.4368, 0.2708, 0.2924],\n",
       "        [0.2366, 0.6202, 0.1432],\n",
       "        [0.3679, 0.4970, 0.1352],\n",
       "        [0.2205, 0.5317, 0.2478],\n",
       "        [0.3503, 0.4686, 0.1811],\n",
       "        [0.4192, 0.4045, 0.1763],\n",
       "        [0.2592, 0.3427, 0.3981],\n",
       "        [0.2891, 0.4259, 0.2849],\n",
       "        [0.3431, 0.5006, 0.1563]], device='cuda:0', grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 148,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(model(input_ids, attention_mask), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9xikRdtRN1N"
   },
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76g7FV85H-T8"
   },
   "source": [
    "Для дообучения модели будем использовать оптимайзер [AdamW](https://huggingface.co/transformers/main_classes/optimizer_schedules.html#adamw) из библиотеки Hugging Face. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ku3RfLMEBYSe"
   },
   "source": [
    "Авторы модели BERT рекомендуют использовать следующие параметры для дообучения модели:\n",
    "- Размер батча: 16, 32\n",
    "- Learning rate (с оптимайзером Adam): 5e-5, 3e-5, 2e-5\n",
    "- Количество эпох: 2, 3, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "executionInfo": {
     "elapsed": 721,
     "status": "ok",
     "timestamp": 1604765399613,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "T5Yo_N-KeEpZ"
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "executionInfo": {
     "elapsed": 768,
     "status": "ok",
     "timestamp": 1604765412756,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "5v-ArJ2fCCcU"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8522g7JIu5J"
   },
   "source": [
    "Реализуем функцию для одной эпохи обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "executionInfo": {
     "elapsed": 826,
     "status": "ok",
     "timestamp": 1604765467312,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "bzl9UhuNx1_Q"
   },
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "  model, \n",
    "  data_loader, \n",
    "  loss_fn, \n",
    "  optimizer, \n",
    "  device, \n",
    "  scheduler, \n",
    "  n_examples\n",
    "):\n",
    "  model = model.train()\n",
    "\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "  \n",
    "  for d in data_loader:\n",
    "    input_ids = d[\"input_ids\"].to(device)\n",
    "    attention_mask = d[\"attention_mask\"].to(device)\n",
    "    targets = d[\"targets\"].to(device)\n",
    "\n",
    "    outputs = model(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask\n",
    "    )\n",
    "\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    loss = loss_fn(outputs, targets)\n",
    "\n",
    "    correct_predictions += torch.sum(preds == targets)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4PniYIte0fr"
   },
   "source": [
    "Также реализуем функцию для оценки качества модели на данных из data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "executionInfo": {
     "elapsed": 621,
     "status": "ok",
     "timestamp": 1604765472066,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "CXeRorVGIKre"
   },
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "  model = model.eval()\n",
    "\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      targets = d[\"targets\"].to(device)\n",
    "\n",
    "      outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "      )\n",
    "      _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "      loss = loss_fn(outputs, targets)\n",
    "\n",
    "      correct_predictions += torch.sum(preds == targets)\n",
    "      losses.append(loss.item())\n",
    "\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_rdSDBHhhCh"
   },
   "source": [
    "Используя эти две функции, описанные выше, реализуем процедуру дообучения нашей модели "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "executionInfo": {
     "elapsed": 696,
     "status": "ok",
     "timestamp": 1604765648276,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "hk9AqJnSe-JE"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2889930,
     "status": "ok",
     "timestamp": 1604768539822,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "1zhHoFNsxufs",
    "outputId": "2e726bfd-5f2d-4614-a196-e90e19c14536"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.5633229434061333 accuracy 0.7715051866487899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 0.5938454067707062 accuracy 0.7725540025412961\n",
      "\n",
      "Epoch 2/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.3007652075870833 accuracy 0.8937971914473221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 0.5345220391452312 accuracy 0.8335451080050826\n",
      "\n",
      "Epoch 3/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.18187011920759752 accuracy 0.944111213040717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 0.6654308759141714 accuracy 0.852604828462516\n",
      "\n",
      "Epoch 4/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.13178786843317156 accuracy 0.9625996753934091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 0.6844161213375628 accuracy 0.8640406607369759\n",
      "\n",
      "Epoch 5/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.09343568444355553 accuracy 0.9734669395243808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 0.7508050329622347 accuracy 0.8703939008894537\n",
      "\n",
      "Epoch 6/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.07480288240573087 accuracy 0.9791828381906711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 0.7019459772488336 accuracy 0.8754764930114358\n",
      "\n",
      "Epoch 7/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.058661474266805465 accuracy 0.9821466374991179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 0.811547315796488 accuracy 0.8818297331639137\n",
      "\n",
      "Epoch 8/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.04614282416395394 accuracy 0.9849693035071626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 0.7748120504170948 accuracy 0.8805590851334181\n",
      "\n",
      "Epoch 9/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.039870151758583425 accuracy 0.9865923364617882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 0.8042915242315212 accuracy 0.8856416772554003\n",
      "\n",
      "Epoch 10/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.033565752391347216 accuracy 0.9874391362642015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val   loss 0.819782407462626 accuracy 0.8818297331639137\n",
      "\n",
      "CPU times: user 34min 19s, sys: 13min 30s, total: 47min 50s\n",
      "Wall time: 48min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "  print('-' * 10)\n",
    "\n",
    "  train_acc, train_loss = train_epoch(\n",
    "    model,\n",
    "    train_data_loader,    \n",
    "    loss_fn, \n",
    "    optimizer, \n",
    "    device, \n",
    "    scheduler, \n",
    "    len(df_train)\n",
    "  )\n",
    "\n",
    "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "  val_acc, val_loss = eval_model(\n",
    "    model,\n",
    "    val_data_loader,\n",
    "    loss_fn, \n",
    "    device, \n",
    "    len(df_val)\n",
    "  )\n",
    "\n",
    "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
    "  print()\n",
    "\n",
    "  history['train_acc'].append(train_acc)\n",
    "  history['train_loss'].append(train_loss)\n",
    "  history['val_acc'].append(val_acc)\n",
    "  history['val_loss'].append(val_loss)\n",
    "\n",
    "  if val_acc > best_accuracy:\n",
    "    torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "    best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4r8-5zWsiVur"
   },
   "source": [
    "\n",
    "При обучении модели мы сохраняем веса наилучшей модели на основе точности на валидационной выборке.\n",
    "\n",
    "Обучение модели занимает некоторое время. Теперь мы можем сравнить точность на валидационной и на обучающей выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "executionInfo": {
     "elapsed": 2402,
     "status": "ok",
     "timestamp": 1604768977612,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "-FWG7kBm372V",
    "outputId": "9dc012e4-1853-432d-e3be-67d848023bfd"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history['train_acc'], label='train accuracy')\n",
    "plt.plot(history['val_acc'], label='validation accuracy')\n",
    "\n",
    "plt.title('Training history')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.ylim([0, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8f3hpdaDnuf"
   },
   "source": [
    "Из графика видно, что точность модели при обучении приближается к 100% после приблизительно 10 эпох."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zoGUH8VZ-pPQ"
   },
   "outputs": [],
   "source": [
    "# !gdown --id 1V8itWtowCYnb2Bc9KlK9SxGff9WwmogA\n",
    "\n",
    "# model = SentimentClassifier(len(class_names))\n",
    "# model.load_state_dict(torch.load('best_model_state.bin'))\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kU9MfvTQC9wh"
   },
   "source": [
    "## Оценка результатов работы модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dnaSaPXMCuBJ"
   },
   "source": [
    "Подсчитаем точность (accuracy) модели на тестовой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6413,
     "status": "ok",
     "timestamp": 1604769007539,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "jS3gJ_qBEljD",
    "outputId": "142c772b-0ffb-4d50-a1f3-59213c1b3208"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8769035532994923"
      ]
     },
     "execution_count": 159,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc, _ = eval_model(\n",
    "  model,\n",
    "  test_data_loader,\n",
    "  loss_fn,\n",
    "  device,\n",
    "  len(df_test)\n",
    ")\n",
    "\n",
    "test_acc.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRfeDJUSDKel"
   },
   "source": [
    "Точность предсказания на тестовых данных примерно на 1% ниже, чем на обучающих. Из этого можно сделать вывод, что получившаяся модель обладает хорошей обобщающей способностью."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1z8MZ3GDZsl"
   },
   "source": [
    "Теперь реализуем вспомогательную функцию, которая будет получать предсказания из нашей модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "executionInfo": {
     "elapsed": 1310,
     "status": "ok",
     "timestamp": 1604769021848,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "EgR6MuNS8jr_"
   },
   "outputs": [],
   "source": [
    "def get_predictions(model, data_loader):\n",
    "  model = model.eval()\n",
    "  \n",
    "  review_texts = []\n",
    "  predictions = []\n",
    "  prediction_probs = []\n",
    "  real_values = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "\n",
    "      texts = d[\"review_text\"]\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      targets = d[\"targets\"].to(device)\n",
    "\n",
    "      outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "      )\n",
    "      _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "      probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "      review_texts.extend(texts)\n",
    "      predictions.extend(preds)\n",
    "      prediction_probs.extend(probs)\n",
    "      real_values.extend(targets)\n",
    "\n",
    "  predictions = torch.stack(predictions).cpu()\n",
    "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
    "  real_values = torch.stack(real_values).cpu()\n",
    "  return review_texts, predictions, prediction_probs, real_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6279,
     "status": "ok",
     "timestamp": 1604769031320,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "zHdPZr60-0c_",
    "outputId": "05e763a3-2a0f-4dd8-d014-e577807e299a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
    "  model,\n",
    "  test_data_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnVDSjRyCK68"
   },
   "source": [
    "Давайте посмотрим на результаты работы модели на тестовой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1184,
     "status": "ok",
     "timestamp": 1604769037281,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "L8a9_8-ND3Is",
    "outputId": "033b9bda-35e3-4340-a79a-bcdb7157b5c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.91      0.91       257\n",
      "     neutral       0.78      0.88      0.83       237\n",
      "    positive       0.94      0.84      0.89       294\n",
      "\n",
      "    accuracy                           0.88       788\n",
      "   macro avg       0.88      0.88      0.88       788\n",
      "weighted avg       0.88      0.88      0.88       788\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4sCG_6yOCHdU"
   },
   "source": [
    "Из данного отчета видно, что сложнее всего классифицировать нейтральные отзывы. \n",
    "\n",
    "Посмотрим также на confusion matrix предсказаний модели на тестовой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "executionInfo": {
     "elapsed": 1297,
     "status": "ok",
     "timestamp": 1604769048397,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "6d1qxsc__DTh",
    "outputId": "b1c4dec3-fd41-4f8d-b49b-de9fe4af84d3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_confusion_matrix(confusion_matrix):\n",
    "  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
    "  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
    "  plt.ylabel('True sentiment')\n",
    "  plt.xlabel('Predicted sentiment');\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "show_confusion_matrix(df_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNoVbpaAChVs"
   },
   "source": [
    "Здесь также можно видеть, что модели сложнее всего классифицировать нейтральные отзывы. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 229
    },
    "executionInfo": {
     "elapsed": 1202,
     "status": "error",
     "timestamp": 1604775058886,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "iANBiY3sLo-K",
    "outputId": "bb15cf2b-81e7-42fa-f655-629392976b11"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b8747c574ac1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mreview_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_review_texts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtrue_sentiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m pred_df = pd.DataFrame({\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_review_texts' is not defined"
     ]
    }
   ],
   "source": [
    "idx = 2\n",
    "\n",
    "review_text = y_review_texts[idx]\n",
    "true_sentiment = y_test[idx]\n",
    "pred_df = pd.DataFrame({\n",
    "  'class_names': class_names,\n",
    "  'values': y_pred_probs[idx]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "executionInfo": {
     "elapsed": 859,
     "status": "error",
     "timestamp": 1604769061201,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "-8D0rb1yfnv4",
    "outputId": "df1d539f-3023-406d-c369-c9dfc2facf88"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-165-8e845f215f19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'True sentiment: {class_names[true_sentiment]}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wrap' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(wrap(review_text)))\n",
    "print()\n",
    "print(f'True sentiment: {class_names[true_sentiment]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WL5pDmvFyaU"
   },
   "source": [
    "## Предсказание на произвольных текстах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1cg2r4SvLrT"
   },
   "source": [
    "Теперь нам осталось научиться использовать нашу модель для предсказания тональности любого текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QEPi7zQRsDhH"
   },
   "outputs": [],
   "source": [
    "review_text = \"I love completing my todos! Best app ever!!!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBSHv23kvXeO"
   },
   "source": [
    "Для использования нашей модели нам следует токенизировать текст соответствующим образом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37072,
     "status": "ok",
     "timestamp": 1604775176555,
     "user": {
      "displayName": "Vasily Chesalov",
      "photoUrl": "",
      "userId": "16540140468090220673"
     },
     "user_tz": -240
    },
    "id": "cUJGpQGuDUNX",
    "outputId": "3f914565-c09a-4edc-cfb1-4304b095350a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zA5Or4D2sLc9"
   },
   "outputs": [],
   "source": [
    "encoded_review = tokenizer.encode_plus(\n",
    "  review_text,\n",
    "  max_length=MAX_LEN,\n",
    "  add_special_tokens=True,\n",
    "  return_token_type_ids=False,\n",
    "  pad_to_max_length=True,\n",
    "  return_attention_mask=True,\n",
    "  return_tensors='pt',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "foCrgUbJvF1R"
   },
   "source": [
    "Теперь получим предсказания нашей модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "Qr_t3rUksumr",
    "outputId": "4a69d750-c56a-40c1-822a-0b3e7df16b3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review text: I love completing my todos! Best app ever!!!\n",
      "Sentiment  : positive\n"
     ]
    }
   ],
   "source": [
    "input_ids = encoded_review['input_ids'].to(device)\n",
    "attention_mask = encoded_review['attention_mask'].to(device)\n",
    "\n",
    "output = model(input_ids, attention_mask)\n",
    "_, prediction = torch.max(output, dim=1)\n",
    "\n",
    "print(f'Review text: {review_text}')\n",
    "print(f'Sentiment  : {class_names[prediction]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmTr_W3gBC7y"
   },
   "source": [
    "## Итоги"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-I4fUlwVBK7f"
   },
   "source": [
    "На этом занятии мы научились использовать модель BERT для классификации текстов по тональности.\n",
    "\n",
    "Мы научились обрабатывать исходные тексты так, чтобы их можно было использовать для обучения модели BERT.\n",
    "\n",
    "Мы построили собственный классификатор с помощью библиотеки Huggig Face и дообучили его на данных отзывов на приложения Google Play.\n",
    "\n",
    "На следующих занятиях мы рассмотрим другие задачи NLP, которые можно решать с помощью моделей, основанных на архитектуре Transformer."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PyTxtM_W6_BERT_classfication_CUDA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
