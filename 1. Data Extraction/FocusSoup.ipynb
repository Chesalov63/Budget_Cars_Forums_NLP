{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "CorollaSoup.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chesalov63/Budget_Cars_Forums_NLP/blob/main/1.%20Data%20Extraction/FocusSoup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "vhwIBamH3A7S"
      },
      "source": [
        "import urllib\n",
        "from urllib.request import urlopen, urlretrieve\n",
        "from urllib.parse import quote_plus\n",
        "from bs4 import BeautifulSoup\n",
        "import regex as re\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "\n",
        "site = 'https://corolla-club.ru/'\n",
        "folder_names = [\n",
        "    ['0', 'Отзывы', 'f=5', 'TOYOTA COROLLA'],\n",
        "    ['1', 'Двигатель', 'f=8', 'TOYOTA COROLLA'],\n",
        "    ['2', 'Трансмиссия', '', 'TOYOTA COROLLA'],\n",
        "    ['3', 'Ходовая', 'f=15', 'TOYOTA COROLLA'],\n",
        "    ['4', 'Колеса', 'f=11', 'TOYOTA COROLLA'],\n",
        "    ['5', 'Электрооборудование', 'f=9', 'TOYOTA COROLLA'],\n",
        "    ['6', 'Мультимедиа', 'f=10', 'TOYOTA COROLLA'],\n",
        "    ['7', 'Кузов', 'f=7', 'TOYOTA COROLLA'],\n",
        "    ['8', 'Жидкости', '', 'TOYOTA COROLLA'],\n",
        "    ['9', 'Сервис', 'f=12', 'TOYOTA COROLLA'],\n",
        "    ['10', 'Эксплуатация', '', 'TOYOTA COROLLA'],\n",
        "    ['11', 'Тюнинг', 'f=32', 'TOYOTA COROLLA'],\n",
        "    ['12', 'Кузов-Вентиляция', 'f=16', 'TOYOTA COROLLA'],\n",
        "    ['13', 'Сервис-Отзывные компании', 'f=31', 'TOYOTA COROLLA'],\n",
        "    ['14', 'Тюнинг-Противоугон', 'f=21', 'TOYOTA COROLLA']\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehRJrzYA3A7a"
      },
      "source": [
        "На странице раздела у нас две задачи:  \n",
        "1. Найти и сохранить все ссылки на темы\n",
        "2. Найти ссылку на следующую страницу раздела или убедиться, что это последняя страница раздела."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "qO8I0BIU3A7b"
      },
      "source": [
        "def collect_themes(site, folder):\n",
        "    \n",
        "    site_address = site + 'forumdisplay.php?' + folder\n",
        "    themes = []\n",
        "    flag = True\n",
        "\n",
        "    while flag:\n",
        "    \n",
        "        resp = urlopen(site_address) # скачиваем файл\n",
        "        html = resp.read().decode('utf-8') # считываем содержимое\n",
        "        soup = BeautifulSoup(html, 'html.parser') # делаем суп\n",
        "    \n",
        "        flag = False\n",
        "        for link in soup.find_all('a'):\n",
        "            if link.has_attr('href') and link.has_attr('id'):\n",
        "                if re.search('thread_title_', str(link.get('id'))):\n",
        "                    s = link.get('href')\n",
        "                    s = site + 'showthread.php?' + re.search('t=[0-9]+', s)[0]\n",
        "                    themes.append([link.get_text(), s])\n",
        "            if not flag and link.has_attr('href') and link.has_attr('rel'):\n",
        "                if link.get('rel')[0] == 'next':\n",
        "                    flag = True\n",
        "                    s = link.get('href')\n",
        "                    site_address = site + 'forumdisplay.php?' + re.search(folder + '[\\S]+', s)[0]\n",
        "    return themes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSkLfsOw3A7b"
      },
      "source": [
        "На странице темы у нас три задачи:  \n",
        "1. Понять, что являеся шапкой темы (это сообщение #1), прочитать ее только один раз - НЕ СДЕЛАНО\n",
        "2. Прочитать и сохранить все сообщения (кроме шапки) на странице темы\n",
        "3. Найти ссылку на следующую страницу темы или убедиться, что это последняя страница темы."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "3wc-StdK3A7c"
      },
      "source": [
        "def collect_messages(site, themes):\n",
        "\n",
        "    messages = []\n",
        "    for theme in tqdm(themes):\n",
        "       \n",
        "        print(theme[0])\n",
        "        theme_address = theme[1]\n",
        "        thread = re.search('t=[0-9]+', theme_address)[0]\n",
        "        flag = True\n",
        "        theme_messages = []\n",
        "        post_data = {}\n",
        "        \n",
        "        while flag:\n",
        "            resp = urlopen(theme_address) # скачиваем файл\n",
        "            html = resp.read().decode('utf-8', errors='ignore') # считываем содержимое\n",
        "            soup = BeautifulSoup(html, 'html.parser') # делаем суп\n",
        "            #Собираем сами сообщения\n",
        "            lis = soup.find_all('li')\n",
        "            for li in lis:\n",
        "                post_id = ''\n",
        "                post_data = {}\n",
        "                if li.has_attr('id') and re.search('post_[0-9]+', li['id']):\n",
        "                    post_id = re.sub('post_', '', li['id'])\n",
        "                    ##Собираем данные о дате и авторе сообщения\n",
        "                    tags = li.find_all('span')\n",
        "                    for span in tags:\n",
        "                        if span.has_attr('class') and span['class'] == ['date']:\n",
        "                            post_data[post_id] = [span.get_text().strip()]\n",
        "                    tags = li.find_all('a')\n",
        "                    author_id = '0'\n",
        "                    author_nickname = 'Guest'\n",
        "                    for tag in tags:\n",
        "                        if tag.has_attr('class') and tag['class'][0] == 'username':\n",
        "                            author_id = re.sub('member.php?u=', '', tag['href'])\n",
        "                            author_nickname = tag.get_text().strip()\n",
        "                    post_data[post_id].append([author_id, author_nickname])\n",
        "                    ##Собираем сами сообщения                            \n",
        "                    tags = li.find_all('div')\n",
        "                    for div in tags:\n",
        "                        if div.has_attr('id') and re.search('post_message_', div['id']):\n",
        "                            blockquote = div.find('blockquote')\n",
        "                            additional_divs = blockquote.find_all('div')\n",
        "                            for a_d in additional_divs:\n",
        "                                a_d.clear()\n",
        "                            s = blockquote.get_text().strip()\n",
        "                            s = re.sub('[\\']+', '`', s)\n",
        "                            s = '\\'' + re.sub('[\\s]+', ' ', s).strip() + '\\''\n",
        "                            if len(s) > 2:\n",
        "                                theme_messages.append([post_data[post_id][0], post_data[post_id][1][1], s])\n",
        "\n",
        "            # В отдельном цикле ищем ссылку на следующую страницу темы (это 'a' с атрибутом 'rel', равным 'next')\n",
        "            # Если ссылка найдена, \n",
        "            flag = False \n",
        "            for link in soup.find_all('a'):\n",
        "                if not flag and link.has_attr('href') and link.has_attr('rel'):\n",
        "                    if link.get('rel')[0] == 'next':\n",
        "                        flag = True\n",
        "                        s = link.get('href')\n",
        "                        theme_address = site + 'showthread.php?' + re.search(thread + '[\\S]+', s)[0]\n",
        "        messages.append(['\\''+ theme[0] + '\\'', theme_messages])\n",
        "    return messages"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "cJp2eySL3A7d"
      },
      "source": [
        "def save_messages(messages, folder_name):\n",
        "    f_name = os.path.join(folder_name[3], folder_name[3] + '_' + folder_name[0] + '_' + folder_name[1] + '.csv')\n",
        "    with open(f_name, 'w', encoding='utf-8') as ouf:\n",
        "        ouf.write('Code,Folder,Theme,Date,Author,Message\\n')\n",
        "        for message in messages:\n",
        "            header = folder_name[:2] + [message[0]]\n",
        "            for item in message[1]:\n",
        "                ouf.write(','.join(header + item))\n",
        "                ouf.write('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "ziGdpCGQ3A7d"
      },
      "source": [
        "def scraper(site, folder_names):\n",
        "    for folder_name in folder_names:\n",
        "        if len(folder_name[2]) > 0:\n",
        "            folder = folder_name[2]\n",
        "            current_themes = collect_themes(site, folder)\n",
        "            current_messages = collect_messages(site, current_themes)\n",
        "            save_messages(current_messages, folder_name)\n",
        "    print('Information collected')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "PpKvW5sm3A7e"
      },
      "source": [
        "scraper(site, folder_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "jMa1anpT3A7e"
      },
      "source": [
        "#import pandas as pd\n",
        "#mess = pd.read_csv('./TOYOTA COROLLA/TOYOTA COROLLA_5_Электрооборудование.csv', quotechar=\"'\")\n",
        "#mess"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "dFwUJ1iz3A7f"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}