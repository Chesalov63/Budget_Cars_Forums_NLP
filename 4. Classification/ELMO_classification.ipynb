{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jcl_u_f_cCC2"
   },
   "source": [
    "<h1><center>Классификация c помощью векторных представлений ELMo</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Ht8lWyYcCC3"
   },
   "source": [
    "На этом занятии мы вернемся к задаче классификации текстов и построим модель классификации отзывов на фильмы на позитивный и негативный классы с помощью векторных представлений ELMo.\n",
    "\n",
    "Мы снова будем использовать [датасет отзывов на фильмы IMDB](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews), с которым мы уже работали на предыдущих занятиях. Каждый из отзывов в датасете имеет свою оценку: является ли он позитивным или негативным.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZVXAKHqcCC6"
   },
   "source": [
    "## Подготовка данных\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NHKLq9hEFhNr"
   },
   "source": [
    "Установим  модули (torchvision и torchtext) и загрузим модель для токенизации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "id": "5kiZro9eG-bG",
    "outputId": "31eb4608-37a6-45e7-9bb5-378d2e410018"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in s:\\anaconda3\\lib\\site-packages (0.7.0+cpu)\n",
      "Requirement already satisfied: numpy in s:\\anaconda3\\lib\\site-packages (from torchvision) (1.19.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in s:\\anaconda3\\lib\\site-packages (from torchvision) (8.0.0)\n",
      "Requirement already satisfied: torch==1.6.0 in s:\\anaconda3\\lib\\site-packages (from torchvision) (1.6.0+cpu)\n",
      "Requirement already satisfied: future in s:\\anaconda3\\lib\\site-packages (from torch==1.6.0->torchvision) (0.18.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "id": "a-WRgs-VFhNt",
    "outputId": "537432a7-d5d8-46db-b400-a36a8dc56496",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext in s:\\anaconda3\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: sentencepiece in s:\\anaconda3\\lib\\site-packages (from torchtext) (0.1.91)\n",
      "Requirement already satisfied: requests in s:\\anaconda3\\lib\\site-packages (from torchtext) (2.24.0)\n",
      "Requirement already satisfied: torch in s:\\anaconda3\\lib\\site-packages (from torchtext) (1.6.0+cpu)\n",
      "Requirement already satisfied: numpy in s:\\anaconda3\\lib\\site-packages (from torchtext) (1.19.1)\n",
      "Requirement already satisfied: tqdm in s:\\anaconda3\\lib\\site-packages (from torchtext) (4.50.2)\n",
      "Requirement already satisfied: six in s:\\anaconda3\\lib\\site-packages (from torchtext) (1.15.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in s:\\anaconda3\\lib\\site-packages (from requests->torchtext) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in s:\\anaconda3\\lib\\site-packages (from requests->torchtext) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in s:\\anaconda3\\lib\\site-packages (from requests->torchtext) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in s:\\anaconda3\\lib\\site-packages (from requests->torchtext) (1.25.10)\n",
      "Requirement already satisfied: future in s:\\anaconda3\\lib\\site-packages (from torch->torchtext) (0.18.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83S_ebUMcCDK"
   },
   "source": [
    "Как мы делали и ранее, загрузим датасет IMDB из torchtext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import en_core_web_sm\n",
    "en_nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "lbQtDL6DcCDL",
    "outputId": "0c5ec619-d8ed-4065-b344-be35b54d2d3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score,accuracy_score,classification_report,confusion_matrix\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter,defaultdict\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "SEED = 0\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "TEXT = data.Field(tokenize='spacy')\n",
    "LABEL = data.LabelField()\n",
    "\n",
    "train_src, test = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7XfPVVL7cCDP"
   },
   "source": [
    "Разделим данные на обучаующую и тестовую выборку и преобразуем их в удобный нам в дальнейшем формат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-SV9I3qycCDQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-365b897e045f>:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  X_train = np.array([example.text for example in train_src])\n",
      "<ipython-input-4-365b897e045f>:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  X_test = np.array([example.text for example in test])\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array([example.text for example in train_src])\n",
    "y_train = np.array([0 if example.label == 'pos' else 1 for example in train_src])\n",
    "\n",
    "X_test = np.array([example.text for example in test])\n",
    "y_test = np.array([0 if example.label == 'pos' else 1 for example in test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "kCeT0NaucCDU",
    "outputId": "b4190b62-174b-4ddd-dbad-9d4acc7bee04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total train examples 25000\n",
      "total test examples 25000\n",
      "12500 positive and 12500 negative examples in test\n",
      "12500 positive and 12500 negative examples in test\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print (\"total train examples %s\" % len(y_train))\n",
    "print (\"total test examples %s\" % len(y_test))\n",
    "\n",
    "train_counter = Counter(y_train)\n",
    "test_counter = Counter(y_test)\n",
    "\n",
    "print (\"{0} positive and {1} negative examples in test\".format(test_counter[1],test_counter[0]))\n",
    "print (\"{0} positive and {1} negative examples in test\".format(test_counter[1],test_counter[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3F1BUnz7cCDY"
   },
   "source": [
    "Можно видеть, что классы сбалансированы: количество позитивных и негативных отзывов совпадают."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6ccN1AlFhNo"
   },
   "source": [
    "## Классификация с использованием векторных представлений ELMo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnrXcH-zcCES"
   },
   "source": [
    "Напомним, что в прошлый раз мы использовали для классификации отзывов сверточную нейронную сеть, а в качестве векторных представлений слов брали предобученные вектора glove."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJ8_zAI8FhNp"
   },
   "source": [
    "Теперь построим классификатор с помощью рекуррентной нейронной сети. Будем использовать для этого слой GRU.\n",
    "\n",
    "Для получения векторных представлений текстов будем использовать предобученную модель ELMo с помощью библиотеки allennlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 980
    },
    "id": "0krWabP5IWYX",
    "outputId": "cc22324d-f8ce-4d94-9452-7a8f6b9a0688",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting allennlp\n",
      "  Downloading allennlp-1.2.0-py3-none-any.whl (498 kB)\n",
      "Requirement already satisfied: scikit-learn in s:\\anaconda3\\lib\\site-packages (from allennlp) (0.23.2)\n",
      "Requirement already satisfied: scipy in s:\\anaconda3\\lib\\site-packages (from allennlp) (1.5.2)\n",
      "Requirement already satisfied: filelock<3.1,>=3.0 in s:\\anaconda3\\lib\\site-packages (from allennlp) (3.0.12)\n",
      "Collecting tensorboardX>=1.2\n",
      "  Downloading tensorboardX-2.1-py2.py3-none-any.whl (308 kB)\n",
      "Collecting overrides==3.1.0\n",
      "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
      "Requirement already satisfied: requests>=2.18 in s:\\anaconda3\\lib\\site-packages (from allennlp) (2.24.0)\n",
      "Requirement already satisfied: torch<1.8.0,>=1.6.0 in s:\\anaconda3\\lib\\site-packages (from allennlp) (1.6.0+cpu)\n",
      "Requirement already satisfied: spacy<2.4,>=2.1.0 in s:\\anaconda3\\lib\\site-packages (from allennlp) (2.3.2)\n",
      "Collecting jsonpickle\n",
      "  Downloading jsonpickle-1.4.1-py2.py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: pytest in s:\\anaconda3\\lib\\site-packages (from allennlp) (0.0.0)\n",
      "Collecting transformers<3.5,>=3.1\n",
      "  Downloading transformers-3.4.0-py3-none-any.whl (1.3 MB)\n",
      "Requirement already satisfied: h5py in s:\\anaconda3\\lib\\site-packages (from allennlp) (2.10.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.14 in s:\\anaconda3\\lib\\site-packages (from allennlp) (1.15.12)\n",
      "Requirement already satisfied: tqdm>=4.19 in s:\\anaconda3\\lib\\site-packages (from allennlp) (4.50.2)\n",
      "Requirement already satisfied: numpy in s:\\anaconda3\\lib\\site-packages (from allennlp) (1.19.1)\n",
      "Requirement already satisfied: nltk in s:\\anaconda3\\lib\\site-packages (from allennlp) (3.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in s:\\anaconda3\\lib\\site-packages (from scikit-learn->allennlp) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in s:\\anaconda3\\lib\\site-packages (from scikit-learn->allennlp) (0.17.0)\n",
      "Requirement already satisfied: six in s:\\anaconda3\\lib\\site-packages (from tensorboardX>=1.2->allennlp) (1.15.0)\n",
      "Collecting protobuf>=3.8.0\n",
      "  Downloading protobuf-3.13.0-py2.py3-none-any.whl (438 kB)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in s:\\anaconda3\\lib\\site-packages (from requests>=2.18->allennlp) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in s:\\anaconda3\\lib\\site-packages (from requests>=2.18->allennlp) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in s:\\anaconda3\\lib\\site-packages (from requests>=2.18->allennlp) (1.25.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in s:\\anaconda3\\lib\\site-packages (from requests>=2.18->allennlp) (2020.6.20)\n",
      "Requirement already satisfied: future in s:\\anaconda3\\lib\\site-packages (from torch<1.8.0,>=1.6.0->allennlp) (0.18.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in s:\\anaconda3\\lib\\site-packages (from spacy<2.4,>=2.1.0->allennlp) (0.8.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in s:\\anaconda3\\lib\\site-packages (from spacy<2.4,>=2.1.0->allennlp) (1.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in s:\\anaconda3\\lib\\site-packages (from spacy<2.4,>=2.1.0->allennlp) (2.0.3)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in s:\\anaconda3\\lib\\site-packages (from spacy<2.4,>=2.1.0->allennlp) (1.1.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in s:\\anaconda3\\lib\\site-packages (from spacy<2.4,>=2.1.0->allennlp) (3.0.2)\n",
      "Requirement already satisfied: thinc==7.4.1 in s:\\anaconda3\\lib\\site-packages (from spacy<2.4,>=2.1.0->allennlp) (7.4.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in s:\\anaconda3\\lib\\site-packages (from spacy<2.4,>=2.1.0->allennlp) (1.0.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in s:\\anaconda3\\lib\\site-packages (from spacy<2.4,>=2.1.0->allennlp) (0.4.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in s:\\anaconda3\\lib\\site-packages (from spacy<2.4,>=2.1.0->allennlp) (1.0.2)\n",
      "Requirement already satisfied: setuptools in s:\\anaconda3\\lib\\site-packages (from spacy<2.4,>=2.1.0->allennlp) (50.3.0)\n",
      "Requirement already satisfied: importlib-metadata in s:\\anaconda3\\lib\\site-packages (from jsonpickle->allennlp) (2.0.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in s:\\anaconda3\\lib\\site-packages (from pytest->allennlp) (20.2.0)\n",
      "Requirement already satisfied: iniconfig in s:\\anaconda3\\lib\\site-packages (from pytest->allennlp) (1.1.1)\n",
      "Requirement already satisfied: packaging in s:\\anaconda3\\lib\\site-packages (from pytest->allennlp) (20.4)\n",
      "Requirement already satisfied: pluggy<1.0,>=0.12 in s:\\anaconda3\\lib\\site-packages (from pytest->allennlp) (0.13.1)\n",
      "Requirement already satisfied: py>=1.8.2 in s:\\anaconda3\\lib\\site-packages (from pytest->allennlp) (1.9.0)\n",
      "Requirement already satisfied: toml in s:\\anaconda3\\lib\\site-packages (from pytest->allennlp) (0.10.1)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in s:\\anaconda3\\lib\\site-packages (from pytest->allennlp) (1.4.0)\n",
      "Requirement already satisfied: colorama in s:\\anaconda3\\lib\\site-packages (from pytest->allennlp) (0.4.3)\n",
      "Collecting tokenizers==0.9.2\n",
      "  Downloading tokenizers-0.9.2-cp38-cp38-win_amd64.whl (1.9 MB)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in s:\\anaconda3\\lib\\site-packages (from transformers<3.5,>=3.1->allennlp) (0.1.91)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in s:\\anaconda3\\lib\\site-packages (from transformers<3.5,>=3.1->allennlp) (2020.10.15)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in s:\\anaconda3\\lib\\site-packages (from boto3<2.0,>=1.14->allennlp) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in s:\\anaconda3\\lib\\site-packages (from boto3<2.0,>=1.14->allennlp) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.19.0,>=1.18.12 in s:\\anaconda3\\lib\\site-packages (from boto3<2.0,>=1.14->allennlp) (1.18.12)\n",
      "Requirement already satisfied: click in s:\\anaconda3\\lib\\site-packages (from nltk->allennlp) (7.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in s:\\anaconda3\\lib\\site-packages (from importlib-metadata->jsonpickle->allennlp) (3.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in s:\\anaconda3\\lib\\site-packages (from packaging->pytest->allennlp) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in s:\\anaconda3\\lib\\site-packages (from botocore<1.19.0,>=1.18.12->boto3<2.0,>=1.14->allennlp) (2.8.1)\n",
      "Building wheels for collected packages: overrides, sacremoses\n",
      "  Building wheel for overrides (setup.py): started\n",
      "  Building wheel for overrides (setup.py): finished with status 'done'\n",
      "  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10179 sha256=829731a622cc81ed35700a64ea4ebbbf8973ea1d3d3913833947a80a4c7d2059\n",
      "  Stored in directory: c:\\users\\basilio\\appdata\\local\\pip\\cache\\wheels\\6a\\4f\\72\\28857f75625b263e2e3f5ab2fc4416c0a85960ac6485007eaa\n",
      "  Building wheel for sacremoses (setup.py): started\n",
      "  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893262 sha256=f9cbf549c5bac404bb36955467b865aaecb79b842b9dd1ddea42be59e333e93a\n",
      "  Stored in directory: c:\\users\\basilio\\appdata\\local\\pip\\cache\\wheels\\7b\\78\\f4\\27d43a65043e1b75dbddaa421b573eddc67e712be4b1c80677\n",
      "Successfully built overrides sacremoses\n",
      "Installing collected packages: protobuf, tensorboardX, overrides, jsonpickle, tokenizers, sacremoses, transformers, allennlp\n",
      "Successfully installed allennlp-1.2.0 jsonpickle-1.4.1 overrides-3.1.0 protobuf-3.13.0 sacremoses-0.0.43 tensorboardX-2.1 tokenizers-0.9.2 transformers-3.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install allennlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5UsRjAV2R4T"
   },
   "source": [
    "Для удобства определим вспомогательный класс Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9WESgu8PFwKM"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class Data(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.data = list(zip(x, y))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert idx < len(self)\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DdH6O9Cgymjd"
   },
   "source": [
    "Опишем класс *RNNEncoder*, который будет представлять собой основную часть модели классификации: на основе векторных представлений ELMo он будет применять рекурентный нейронный слой GRU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "QXwQviMlSGCo"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class RNNEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, out_size):\n",
    "        '''\n",
    "        input_size\n",
    "        hidden_size: the output size of CNN/RNN/TR\n",
    "        outpu_size: the final size of the encoder (after pooling)\n",
    "        w\n",
    "        CNN:\n",
    "        - filters_num: feature_dim\n",
    "        - filter_size: 3\n",
    "        - pooling: max_pooling\n",
    "        RNN:\n",
    "        - hidden_size: feature_dim // 2\n",
    "        - pooling: last hidden status\n",
    "        Transformer\n",
    "        - nhead: 2\n",
    "        - nlayer: 1\n",
    "        - pooling: average\n",
    "        -------\n",
    "        '''\n",
    "        super(RNNEncoder, self).__init__()\n",
    "        \n",
    "        self.rnn = nn.GRU(input_size, hidden_size//2, batch_first=True, bidirectional=True)\n",
    "        f_dim = hidden_size\n",
    "\n",
    "        self.fc = nn.Linear(f_dim, out_size)\n",
    "        nn.init.uniform_(self.fc.weight, -0.5, 0.5)\n",
    "        nn.init.uniform_(self.fc.bias, -0.1, 0.1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        out, _ = self.rnn(inputs)\n",
    "        return self.fc(out.mean(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3O1QO8Jz0DZ"
   },
   "source": [
    "Опишем класс ElmoModel. Основной функционал класса: \n",
    "- для поступившего на вход текста получить его векторное представление с помощью модели ELMo \n",
    "- применить к получившимся векторам RNNEncoder\n",
    "- применить линейный слой размерности  *batch_size* x *num_label* (для классификации)\n",
    "\n",
    "Предобученная модель ELMo будет загружена при инициализации соответствующего класса *allennlp.modules.elmo*. Для этого нужно передать две ссылки $-$ ссылки на файлы с параметрами (*elmo_options_file*) и с весами (*elmo_weight_file*) модели. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "EJbzrvl2R2_L"
   },
   "outputs": [],
   "source": [
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ElmoModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 use_gpu=True,\n",
    "                 device=\"cuda:0\",\n",
    "                 out_size=64,\n",
    "                 num_labels=2,\n",
    "                 hidden_size=100,\n",
    "                 dropout=0.5,\n",
    "                 elmo_options_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\",\n",
    "                 elmo_weight_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\",\n",
    "                 elmo_dim=512,):\n",
    "\n",
    "        super(ElmoModel, self).__init__()\n",
    "        self.use_gpu = use_gpu\n",
    "        self.word_dim = elmo_dim\n",
    "        self.device = device\n",
    "        self.elmo_options_file = elmo_options_file\n",
    "        self.elmo_weight_file = elmo_weight_file\n",
    "        self.init_elmo()\n",
    "        \n",
    "        self.encoder = RNNEncoder(self.word_dim, hidden_size, out_size)\n",
    "        self.cls = nn.Linear(out_size, num_labels)\n",
    "        nn.init.uniform_(self.cls.weight, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.cls.bias, -0.1, 0.1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        word_embs = self.get_elmo(x)\n",
    "\n",
    "        x = self.encoder(word_embs)\n",
    "        x = self.dropout(x)\n",
    "        x = self.cls(x)    # batch_size * num_label\n",
    "        return x\n",
    "\n",
    "    def init_elmo(self):\n",
    "        '''\n",
    "        initilize the ELMo model\n",
    "        '''\n",
    "        self.elmo = Elmo(self.elmo_options_file, self.elmo_weight_file, 1)\n",
    "        for param in self.elmo.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "    def get_elmo(self, sentence_lists):\n",
    "        '''\n",
    "        get the ELMo word embedding vectors for a sentences\n",
    "        '''\n",
    "        character_ids = batch_to_ids(sentence_lists)\n",
    "        if self.use_gpu:\n",
    "            character_ids = character_ids.to(self.device)\n",
    "        embeddings = self.elmo(character_ids)\n",
    "        return embeddings['elmo_representations'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJrR98ha2bhf"
   },
   "source": [
    "Определим все необходимые для нашей модели параметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "tHzPVFl5SJ3Q"
   },
   "outputs": [],
   "source": [
    "# ELMo\n",
    "elmo_options_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
    "elmo_weight_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n",
    "\n",
    "elmo_dim = 1024\n",
    "\n",
    "hidden_size = 200\n",
    "out_size = 64\n",
    "num_labels = 2\n",
    "\n",
    "use_gpu = False\n",
    "seed = 2020\n",
    "gpu_id = 0\n",
    "\n",
    "dropout = 0.5\n",
    "epochs = 15\n",
    "\n",
    "test_size = 0.1\n",
    "lr = 1e-3\n",
    "weight_decay = 1e-4\n",
    "batch_size = 8\n",
    "device = \"cpu\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZivMqtU2gYZ"
   },
   "source": [
    "Реализуем процедуру обучения нейронной сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "1_RLpfa8SKGw"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def now():\n",
    "    return str(time.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    data, label = zip(*batch)\n",
    "    return data, label\n",
    "\n",
    "def train(x_train,\n",
    "          x_test,\n",
    "          y_train,\n",
    "          y_test,\n",
    "          seed=seed,\n",
    "          use_gpu=True,\n",
    "          batch_size=batch_size,\n",
    "          test_size=test_size,\n",
    "          lr=lr,\n",
    "          epochs=epochs,\n",
    "          weight_decay=weight_decay):\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if use_gpu:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "    train_data = Data(x_train, y_train)\n",
    "    test_data = Data(x_test, y_test)\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    model = ElmoModel(use_gpu=True,\n",
    "                      device=device,\n",
    "                      out_size=out_size,\n",
    "                      num_labels=num_labels,\n",
    "                      hidden_size=hidden_size,\n",
    "                      dropout=0.5,\n",
    "                      elmo_options_file=elmo_options_file,\n",
    "                      elmo_weight_file=elmo_weight_file,\n",
    "                      elmo_dim=elmo_dim)\n",
    "    print(f\"{now()} Elmo init model finished\")\n",
    "\n",
    "    if use_gpu:\n",
    "        model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    lr_sheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.7)\n",
    "    best_acc = -0.1\n",
    "    best_epoch = -1\n",
    "    start_time = time.time()\n",
    "    for epoch in range(1, epochs):\n",
    "        total_loss = 0.0\n",
    "        model.train()\n",
    "        for step, batch_data in enumerate(train_loader):\n",
    "            # print(batch_data)\n",
    "            x, labels = batch_data\n",
    "            labels = torch.LongTensor(labels)\n",
    "            if use_gpu:\n",
    "                labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # return model,x\n",
    "            output = model(x)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        acc = test(model, test_loader)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_epoch = epoch\n",
    "        print(f\"{now()} Epoch{epoch}: loss: {total_loss}, test_acc: {acc}\")\n",
    "        lr_sheduler.step()\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"*\"*20)\n",
    "    print(f\"{now()} finished; epoch {best_epoch} best_acc: {best_acc}, time/epoch: {(end_time-start_time)/epochs}\")\n",
    "\n",
    "\n",
    "def test(model, test_loader):\n",
    "    correct = 0\n",
    "    num = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            x, labels = data\n",
    "            num += len(labels)\n",
    "            output = model(x)\n",
    "            labels = torch.LongTensor(labels)\n",
    "            if use_gpu:\n",
    "                output = output.cpu()\n",
    "            predict = torch.max(output.data, 1)[1]\n",
    "            correct += (predict == labels).sum().item()\n",
    "    model.train()\n",
    "    return correct * 1.0 / num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext ipytelegram\n",
    "%reload_ext ipytelegram\n",
    "import telepot\n",
    "bot = telepot.Bot('1301715666:AAGBzLlVDZI7KzGZ_DNyukjauVeTt0QpO-A')\n",
    "response = bot.getUpdates()\n",
    "%telegram_setup 1301715666:AAGBzLlVDZI7KzGZ_DNyukjauVeTt0QpO-A 1305740495"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 346
    },
    "id": "iukYHbxKn8Ih",
    "outputId": "c2d4e3d0-1cb2-498d-9490-a387061a7101"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-07 00:47:47 Elmo init model finished\n"
     ]
    }
   ],
   "source": [
    "%%telegram_send ELMo_training_report\n",
    "train(X_train,X_test,y_train,y_test, use_gpu=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QG4sdoXlU-cJ"
   },
   "source": [
    "Как видим, качество работы модели на этом же наборе данных выросло по сравнению с нейронной сетью CNN на векторах glove. Напомним, что максимальное качество классификации, которого удалось достичь тогда, было около 87%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KX3QTdF4cCEx"
   },
   "source": [
    "## Итоги\n",
    "На этом занятии мы научились применять векторные представления ELMo в задаче бинарной классификации текстов.\n",
    "Мы научились работать с библиотекой allennlp, чтобы получить предобученные вектора ELMo и использовали их для построение реккурентной нейронной сети (на основе GRU) для решения нужной задачи. \n",
    "\n",
    "Таким образом мы повысили качество классификации с 87% до 89% (по сравнению с моделью CNN на векторах glove).\n",
    "\n",
    "На следующих занятиях мы познакомимся с моделями на основе архитектуры Transformer и научимся применять их в решении разных задач NLP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gqgX6oHUcCEx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "7_elmo_classification",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
